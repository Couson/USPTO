{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# nltk.download(\"punkt\")\n",
    "\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGSTOP = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. peek 2 json objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !head -n 2 sample_from_full.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# b = db.read_text('2008.json').map(json.loads)\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_needed = ['patentCaseMetadata.applicationNumberText.value',\n",
    "               'patentCaseMetadata.applicationNumberText.electronicText',\n",
    "               'patentCaseMetadata.filingDate',\n",
    "               'patentCaseMetadata.applicationTypeCategory',\n",
    "               'patentCaseMetadata.inventionTitle.content',\n",
    "               'patentCaseMetadata.groupArtUnitNumber.value',\n",
    "               'patentCaseMetadata.groupArtUnitNumber.electronicText',\n",
    "               'patentCaseMetadata.applicationConfirmationNumber',\n",
    "               'patentCaseMetadata.firstInventorToFileIndicator',\n",
    "               'patentCaseMetadata.inventionTitle.content',\n",
    "               'patentCaseMetadata.applicationStatusCategory',\n",
    "               'patentCaseMetadata.applicationStatusDate',\n",
    "               'patentCaseMetadata.officialFileLocationCategory',\n",
    "               'patentCaseMetadata.patentClassificationBag.cpcClassificationBagOrIPCClassificationOrECLAClassificationBag',\n",
    "               'patentCaseMetadata.patentGrantIdentification.patentNumber',\n",
    "               'patentCaseMetadata.patentGrantIdentification.grantDate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(fp):\n",
    "    with open(fp, 'r') as file:\n",
    "        for i in file:\n",
    "            yield i\n",
    "\n",
    "def clean_df(df):\n",
    "    out = []\n",
    "    for col in cols_needed:\n",
    "        try:\n",
    "            to_append = df.loc[:, col].values[0]\n",
    "            out.append(to_append)\n",
    "        except:\n",
    "            out.append(np.NaN)\n",
    "    return out\n",
    "\n",
    "def str2json(json_str):\n",
    "    json_file = json.loads(json_str)\n",
    "    \n",
    "    return json_file\n",
    "\n",
    "def json2df(json_file):\n",
    "    norm = pd.json_normalize(json_file)\n",
    "    out = clean_df(norm)\n",
    "#     print(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "def json_parser(json_path, num_iter = 1):\n",
    "    idx = 0\n",
    "    out = []\n",
    "    with open('meta_data.csv', 'a') as meta_data:\n",
    "        for i in read_text(json_path):\n",
    "    #         print(i)\n",
    "            if idx == 0:\n",
    "                idx += 1\n",
    "                continue\n",
    "            elif idx > num_iter:\n",
    "                break\n",
    "            else: \n",
    "                if i[0] == ',':\n",
    "                    json_str = i[1:]\n",
    "                else:\n",
    "                    json_str = i\n",
    "    #             print(json_str)\n",
    "                out = json2df(str2json(json_str))\n",
    "                to_write = str(out)[1:-1]\n",
    "                print(to_write)\n",
    "            meta_data.write(to_write + '\\n')\n",
    "            idx += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. call this cell given __num__ of iterations to output the cleaned csv\n",
    "- TODO further clean the nested list; get the nested values\n",
    "- TODO: create a header for the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# json_parser('2008.json', 10)\n",
    "# !head -n 1 sample_from_full.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detailedDescription = get_field_values(data, 'abstract')\n",
    "# detailedDescription\n",
    "# json.loads(data)['abstract']\n",
    "# frequency_dict(detailedDescription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(data):\n",
    "    PATTERN = \"applicationStatusCategory\\\":\\\"([\\w+|\\d+|\\s|-]+)\"\n",
    "    return re.findall(PATTERN, data)\n",
    "            \n",
    "def get_field_values(data, field):\n",
    "    # 'abstract', 'detailedDescription', 'summaryOfInvention'\n",
    "    # [para.values() for para in json.loads(data)['detailedDescription']\n",
    "    out = ''\n",
    "    try:\n",
    "        for val in json.loads(data)[field]:\n",
    "    #         print(val)\n",
    "            try:\n",
    "                out += val['text'].lower()\n",
    "            except:\n",
    "                out += val[0].lower()\n",
    "    except:\n",
    "        return out\n",
    "#         print('no %s found'  %field)\n",
    "    return out\n",
    "\n",
    "def frequency_dict(text, out , stopwords = ENGSTOP):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    for w in words:\n",
    "        word_stem = stemmer.stem(w)\n",
    "        if (word_stem not in stopwords) and (word_stem.isalnum()):\n",
    "            out[word_stem] += 1\n",
    "    \n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build bag of words - write all text set to local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('full_text.txt', 'a') as full_text_file:\n",
    "    \n",
    "    for data in read_text('sample_from_full.json'):\n",
    "    #     print(data)    \n",
    "        label = get_label(data)\n",
    "        if len(label) == 1:\n",
    "            UNIQUE_LABELS[label[0]] += 1\n",
    "            valid += 1\n",
    "\n",
    "\n",
    "            abstract = get_field_values(data, 'abstract')\n",
    "            detailedDescription = get_field_values(data, 'detailedDescription')\n",
    "            summaryOfInvention = get_field_values(data, 'summaryOfInvention')\n",
    "            full_text = abstract + ' ' + detailedDescription + ' ' + summaryOfInvention + '\\n'\n",
    "            full_text_file.write(full_text)\n",
    "\n",
    "        elif len(label) == 0:\n",
    "            err += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build bag of words - preform frequency_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-3da2933891ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'full_text.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mbags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrequency_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mbags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-176-dbd0da557b4a>\u001b[0m in \u001b[0;36mfrequency_dict\u001b[0;34m(text, out, stopwords)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mword_stem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword_stem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword_stem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalnum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_stem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step5a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step5b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_step4\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    598\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0;34m'ous'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasure_gt_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0;34m'ive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasure_gt_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0;34m'ize'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasure_gt_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m             ],\n\u001b[1;32m    602\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_apply_rule_list\u001b[0;34m(self, word, rules)\u001b[0m\n\u001b[1;32m    265\u001b[0m                     \u001b[0;31m# Don't try any further rules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m                 \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcondition\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bags = defaultdict(int)\n",
    "for line in read_text('full_text.txt'):\n",
    "    bags = frequency_dict(line, bags)\n",
    "bags    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['may',\n",
       " 'embodi',\n",
       " 'one',\n",
       " 'devic',\n",
       " 'includ',\n",
       " 'exampl',\n",
       " 'use',\n",
       " 'first',\n",
       " '1',\n",
       " 'fig',\n",
       " '2',\n",
       " 'second',\n",
       " 'data',\n",
       " 'system',\n",
       " 'control',\n",
       " 'provid',\n",
       " 'thi',\n",
       " 'process',\n",
       " 'present',\n",
       " 'form',\n",
       " 'signal',\n",
       " 'portion',\n",
       " 'layer',\n",
       " 'user',\n",
       " 'unit',\n",
       " '3',\n",
       " 'invent',\n",
       " 'inform',\n",
       " 'method',\n",
       " 'oper',\n",
       " 'gener',\n",
       " 'describ',\n",
       " 'imag',\n",
       " 'compris',\n",
       " 'least',\n",
       " 'wa',\n",
       " 'base',\n",
       " 'configur',\n",
       " 'also',\n",
       " 'surfac',\n",
       " 'time',\n",
       " 'b',\n",
       " 'ani',\n",
       " 'receiv',\n",
       " 'posit',\n",
       " 'ha',\n",
       " 'determin',\n",
       " 'refer',\n",
       " '10',\n",
       " 'connect',\n",
       " 'accord',\n",
       " 'shown',\n",
       " 'materi',\n",
       " 'commun',\n",
       " 'element',\n",
       " '4',\n",
       " 'perform',\n",
       " 'comput',\n",
       " 'group',\n",
       " 'display',\n",
       " 'step',\n",
       " 'cell',\n",
       " 'compon',\n",
       " 'valu',\n",
       " 'c',\n",
       " 'select',\n",
       " 'differ',\n",
       " 'set',\n",
       " 'prefer',\n",
       " '5',\n",
       " 'illustr',\n",
       " 'applic',\n",
       " '100',\n",
       " 'modul',\n",
       " 'network',\n",
       " 'acid',\n",
       " 'direct',\n",
       " 'structur',\n",
       " 'light',\n",
       " 'addit',\n",
       " 'power',\n",
       " 'end',\n",
       " 'line',\n",
       " '1h',\n",
       " 'region',\n",
       " 'within',\n",
       " 'number',\n",
       " 'member',\n",
       " 'anoth',\n",
       " 'function',\n",
       " 'two',\n",
       " 'abov',\n",
       " 'part',\n",
       " 'herein',\n",
       " '20',\n",
       " 'side',\n",
       " 'correspond',\n",
       " 'output',\n",
       " 'indic',\n",
       " 'r',\n",
       " 'memori',\n",
       " '6',\n",
       " 'detect',\n",
       " 'measur',\n",
       " 'contain',\n",
       " 'case',\n",
       " 'compound',\n",
       " 'result',\n",
       " 'specif',\n",
       " 'block',\n",
       " 'limit',\n",
       " '12',\n",
       " 'respect',\n",
       " 'input',\n",
       " 'plural',\n",
       " 'locat',\n",
       " 'section',\n",
       " 'activ',\n",
       " 'combin',\n",
       " 'apparatu',\n",
       " 'circuit',\n",
       " 'sensor',\n",
       " 'store',\n",
       " 'implement',\n",
       " 'support',\n",
       " 'composit',\n",
       " 'sourc',\n",
       " 'type',\n",
       " 'temperatur',\n",
       " 'associ',\n",
       " 'variou',\n",
       " 'point',\n",
       " 'electr',\n",
       " 'storag',\n",
       " 'area',\n",
       " 'content',\n",
       " 'termin',\n",
       " 'electrod',\n",
       " 'current',\n",
       " 'access',\n",
       " '30',\n",
       " 'flow',\n",
       " 'object',\n",
       " 'wherein',\n",
       " 'bodi',\n",
       " 'transmit',\n",
       " 'sequenc',\n",
       " 'coupl',\n",
       " 'channel',\n",
       " 'particular',\n",
       " 'state',\n",
       " 'follow',\n",
       " 'obtain',\n",
       " 'via',\n",
       " 'hz',\n",
       " 'substrat',\n",
       " 'increas',\n",
       " 'server',\n",
       " 'processor',\n",
       " 'sampl',\n",
       " 'chang',\n",
       " 'product',\n",
       " 'electron',\n",
       " 'relat',\n",
       " 'voltag',\n",
       " 'rotat',\n",
       " 'servic',\n",
       " 'thu',\n",
       " 'amount',\n",
       " 'interfac',\n",
       " 'drive',\n",
       " '7',\n",
       " '110',\n",
       " 'appli',\n",
       " 'identifi',\n",
       " '0',\n",
       " 'level',\n",
       " '8',\n",
       " 'rang',\n",
       " 'n',\n",
       " 'contact',\n",
       " 'heat',\n",
       " 'disclosur',\n",
       " 'onli',\n",
       " 'order',\n",
       " 'allow',\n",
       " 'aspect',\n",
       " 'defin',\n",
       " 'open',\n",
       " 'effect',\n",
       " 'transmiss',\n",
       " 'vehicl',\n",
       " 'less',\n",
       " 'third',\n",
       " 'dure',\n",
       " '14',\n",
       " 'mean',\n",
       " 'frequenc',\n",
       " 'film',\n",
       " 'reduc',\n",
       " 'solut',\n",
       " 'frame',\n",
       " 'lower',\n",
       " 'featur',\n",
       " 'program',\n",
       " 'high',\n",
       " 'execut',\n",
       " 'field',\n",
       " 'target',\n",
       " 'like',\n",
       " '50',\n",
       " '11',\n",
       " 'respons',\n",
       " 'water',\n",
       " 'condit',\n",
       " 'filter',\n",
       " 'node',\n",
       " 'separ',\n",
       " 'assembl',\n",
       " 'show',\n",
       " '102',\n",
       " 'repres',\n",
       " 'switch',\n",
       " 'view',\n",
       " 'g',\n",
       " '200',\n",
       " 'metal',\n",
       " 'arrang',\n",
       " 'plate',\n",
       " 'extend',\n",
       " 'optic',\n",
       " 'thereof',\n",
       " 'ad',\n",
       " 'mobil',\n",
       " 'rel',\n",
       " 'suitabl',\n",
       " 'altern',\n",
       " 'mode',\n",
       " '120',\n",
       " 'pressur',\n",
       " 'instruct',\n",
       " 'wireless',\n",
       " 'j',\n",
       " 'size',\n",
       " 'h',\n",
       " 'requir',\n",
       " 'weight',\n",
       " 'request',\n",
       " 'remov',\n",
       " 'detail',\n",
       " '16',\n",
       " 'exemplari',\n",
       " 'shape',\n",
       " 'without',\n",
       " 'adjust',\n",
       " 'possibl',\n",
       " 'conduct',\n",
       " 'manag',\n",
       " 'reaction',\n",
       " 'fluid',\n",
       " 'semiconductor',\n",
       " 'agent',\n",
       " 'term',\n",
       " 'howev',\n",
       " 'video',\n",
       " 'code',\n",
       " '40',\n",
       " 'id',\n",
       " '9',\n",
       " 'multipl',\n",
       " 'period',\n",
       " 'well',\n",
       " 'messag',\n",
       " 'space',\n",
       " 'tabl',\n",
       " '15',\n",
       " 'art',\n",
       " 'move',\n",
       " 'suppli',\n",
       " 'desir',\n",
       " '22',\n",
       " 'wire',\n",
       " 'made',\n",
       " 'mechan',\n",
       " 'along',\n",
       " 'mixtur',\n",
       " 'wall',\n",
       " 'compar',\n",
       " 'magnet',\n",
       " 'descript',\n",
       " 'singl',\n",
       " 'x',\n",
       " 'produc',\n",
       " 'upper',\n",
       " 'hous',\n",
       " 'length',\n",
       " 'p',\n",
       " 'resist',\n",
       " 'rate',\n",
       " 'caus',\n",
       " '2h',\n",
       " 'ml',\n",
       " 'ga',\n",
       " '18',\n",
       " 'paramet',\n",
       " 'resourc',\n",
       " 'transfer',\n",
       " 'gate',\n",
       " 'option',\n",
       " 'outer',\n",
       " 'phase',\n",
       " 'dispos',\n",
       " 'certain',\n",
       " 'calcul',\n",
       " 'attach',\n",
       " 'medium',\n",
       " '104',\n",
       " '13',\n",
       " 'substanti',\n",
       " 'treatment',\n",
       " 'organ',\n",
       " 'cover',\n",
       " 'need',\n",
       " 'concentr',\n",
       " '24',\n",
       " 'subject',\n",
       " 'design',\n",
       " 'prepar',\n",
       " 'similar',\n",
       " 'low',\n",
       " 'edg',\n",
       " 'patient',\n",
       " 'inner',\n",
       " '300',\n",
       " 'whether',\n",
       " 'pattern',\n",
       " 'encod',\n",
       " 'known',\n",
       " 'greater',\n",
       " 'sheet',\n",
       " 'oxid',\n",
       " 'carbon',\n",
       " 'initi',\n",
       " 'test',\n",
       " 'disclos',\n",
       " 'place',\n",
       " 'event',\n",
       " 'guid',\n",
       " 'valv',\n",
       " 'et',\n",
       " 'therefor',\n",
       " 'improv',\n",
       " 'liquid',\n",
       " 'formula',\n",
       " 'insert',\n",
       " 'mount',\n",
       " 'instanc',\n",
       " 'record',\n",
       " 'mm',\n",
       " 'resin',\n",
       " 'angl',\n",
       " 'pixel',\n",
       " 'media',\n",
       " 'protein',\n",
       " '60',\n",
       " 'way',\n",
       " 'load',\n",
       " 'engin',\n",
       " 'monitor',\n",
       " 'mg',\n",
       " 'said',\n",
       " 'axi',\n",
       " 'l',\n",
       " 'particl',\n",
       " 'integr',\n",
       " '21',\n",
       " 'polym',\n",
       " 'amino',\n",
       " 'panel',\n",
       " 'hole',\n",
       " 'path',\n",
       " 'etc',\n",
       " 'manner',\n",
       " 'air',\n",
       " 'camera',\n",
       " 'forc',\n",
       " 'predetermin',\n",
       " 'manufactur',\n",
       " 'link',\n",
       " '130',\n",
       " 'station',\n",
       " 'creat',\n",
       " 'distanc',\n",
       " '25',\n",
       " 'main',\n",
       " 'ratio',\n",
       " '101',\n",
       " 'color',\n",
       " 'util',\n",
       " 'express',\n",
       " 'sens',\n",
       " 'typic',\n",
       " 'f',\n",
       " 'machin',\n",
       " 'batteri',\n",
       " 'charg',\n",
       " 'file',\n",
       " '32',\n",
       " 'capabl',\n",
       " 'transistor',\n",
       " 'insul',\n",
       " 'face',\n",
       " 'secur',\n",
       " 'format',\n",
       " '106',\n",
       " 'coat',\n",
       " 'virtual',\n",
       " 'nucleic',\n",
       " 'extract',\n",
       " 'model',\n",
       " 'individu',\n",
       " 'carrier',\n",
       " 'client',\n",
       " 'plant',\n",
       " 'alkyl',\n",
       " 'distribut',\n",
       " 'port',\n",
       " 'screen',\n",
       " 'substitut',\n",
       " 'softwar',\n",
       " 'mmol',\n",
       " 'doe',\n",
       " 'touch',\n",
       " 'read',\n",
       " 'extern',\n",
       " 'prevent',\n",
       " 'energi',\n",
       " 'new',\n",
       " 'seal',\n",
       " 'bit',\n",
       " 'physic',\n",
       " '112',\n",
       " 'front',\n",
       " 'motor',\n",
       " 'collect',\n",
       " 'equal',\n",
       " 'skill',\n",
       " 'bottom',\n",
       " 'ue',\n",
       " '400',\n",
       " 'threshold',\n",
       " 'array',\n",
       " 'top',\n",
       " 'techniqu',\n",
       " 'thick',\n",
       " 'occur',\n",
       " 'enabl',\n",
       " 'work',\n",
       " 'consist',\n",
       " 'antenna',\n",
       " 'digit',\n",
       " 'captur',\n",
       " 'fix',\n",
       " 'therebi',\n",
       " 'modifi',\n",
       " 'e',\n",
       " 'local',\n",
       " 'seq',\n",
       " 'avail',\n",
       " 'make',\n",
       " 'core',\n",
       " 'document',\n",
       " 'shaft',\n",
       " 'carri',\n",
       " 'characterist',\n",
       " 'three',\n",
       " 'search',\n",
       " 'depend',\n",
       " 'solid',\n",
       " 'higher',\n",
       " 'logic',\n",
       " 'continu',\n",
       " 'send',\n",
       " 'speed',\n",
       " 'complet',\n",
       " 'segment',\n",
       " 'address',\n",
       " '210',\n",
       " 'close',\n",
       " 'protect',\n",
       " '150',\n",
       " 'atom',\n",
       " 'note',\n",
       " 'person',\n",
       " 'command',\n",
       " 'beam',\n",
       " 'total',\n",
       " '140',\n",
       " 'pass',\n",
       " 'stage',\n",
       " 'even',\n",
       " 'start',\n",
       " 'reflect',\n",
       " 'dri',\n",
       " 'upon',\n",
       " 'would',\n",
       " 'tube',\n",
       " '500',\n",
       " 'index',\n",
       " '31',\n",
       " 'vector',\n",
       " 'len',\n",
       " 'head',\n",
       " 'given',\n",
       " 'understood',\n",
       " '26',\n",
       " 'either',\n",
       " 'v',\n",
       " 'diagram',\n",
       " 'cool',\n",
       " 'befor',\n",
       " 'scope',\n",
       " 'pair',\n",
       " 'claim',\n",
       " '23',\n",
       " 'due',\n",
       " 'advantag',\n",
       " 'print',\n",
       " 'item',\n",
       " 'column',\n",
       " 'environ',\n",
       " 'right',\n",
       " 'properti',\n",
       " 'nm',\n",
       " 'tool',\n",
       " 'movement',\n",
       " 'map',\n",
       " 'chamber',\n",
       " 'incorpor',\n",
       " 'lead',\n",
       " 'oil',\n",
       " 'ring',\n",
       " 'bond',\n",
       " 'engag',\n",
       " 'back',\n",
       " 'particularli',\n",
       " 'vertic',\n",
       " 'deriv',\n",
       " 'k',\n",
       " 'chain',\n",
       " 'intend',\n",
       " 'turn',\n",
       " 'tissu',\n",
       " 'primari',\n",
       " 'maintain',\n",
       " 'purpos',\n",
       " '17',\n",
       " 'treat',\n",
       " 'width',\n",
       " 'sinc',\n",
       " 'achiev',\n",
       " 'diamet',\n",
       " 'intern',\n",
       " 'arm',\n",
       " 'depict',\n",
       " 'fiber',\n",
       " 'could',\n",
       " 'adjac',\n",
       " 'see',\n",
       " 'list',\n",
       " 'proxim',\n",
       " 'becaus',\n",
       " 'hardwar',\n",
       " 'per',\n",
       " 'stream',\n",
       " 'central',\n",
       " 'profil',\n",
       " 'zone',\n",
       " '108',\n",
       " 'accordingli',\n",
       " 'puls',\n",
       " 'correct',\n",
       " 'host',\n",
       " 'domain',\n",
       " 'specifi',\n",
       " 'estim',\n",
       " '202',\n",
       " 'degre',\n",
       " 'standard',\n",
       " '80',\n",
       " 'databas',\n",
       " 'radio',\n",
       " 'motion',\n",
       " 'vessel',\n",
       " 'equip',\n",
       " 'thermal',\n",
       " 'call',\n",
       " 'actuat',\n",
       " 'emit',\n",
       " 'mass',\n",
       " 'patent',\n",
       " 'distal',\n",
       " 'convert',\n",
       " '70',\n",
       " '28',\n",
       " 'key',\n",
       " 'align',\n",
       " 'prior',\n",
       " 'cut',\n",
       " 'μm',\n",
       " 'site',\n",
       " 'procedur',\n",
       " 'card',\n",
       " 'left',\n",
       " 'chemic',\n",
       " 'lock',\n",
       " 'driver',\n",
       " '114',\n",
       " 'releas',\n",
       " 'decod',\n",
       " '3h',\n",
       " 'antibodi',\n",
       " 'enhanc',\n",
       " 'remot',\n",
       " 'scan',\n",
       " '34',\n",
       " 'word',\n",
       " 'interact',\n",
       " 'transact',\n",
       " 'center',\n",
       " 'develop',\n",
       " 'construct',\n",
       " 'analysi',\n",
       " 'remain',\n",
       " 'directli',\n",
       " 'later',\n",
       " 'hour',\n",
       " 'discuss',\n",
       " 'band',\n",
       " 'parallel',\n",
       " 'buffer',\n",
       " 'normal',\n",
       " 'fourth',\n",
       " 'appropri',\n",
       " 'partial',\n",
       " 'glass',\n",
       " 'day',\n",
       " 'acet',\n",
       " 'intermedi',\n",
       " 'adapt',\n",
       " 'bind',\n",
       " 'factor',\n",
       " 'decreas',\n",
       " '42',\n",
       " 'small',\n",
       " 'project',\n",
       " 'modif',\n",
       " 'pat',\n",
       " 'volum',\n",
       " 'technolog',\n",
       " 'plane',\n",
       " 'toward',\n",
       " 'residu',\n",
       " 'transform',\n",
       " '90',\n",
       " '105',\n",
       " 'mix',\n",
       " 'becom',\n",
       " 'averag',\n",
       " 'match',\n",
       " 'independ',\n",
       " 'instal',\n",
       " 'maximum',\n",
       " 'packet',\n",
       " 'name',\n",
       " 'solvent',\n",
       " 'report',\n",
       " 'larg',\n",
       " 'otherwis',\n",
       " 'transport',\n",
       " 'account',\n",
       " 'connector',\n",
       " 'draw',\n",
       " 'minut',\n",
       " 'molecul',\n",
       " 'togeth',\n",
       " 'human',\n",
       " 'mhz',\n",
       " 'w',\n",
       " 'protocol',\n",
       " 'mani',\n",
       " 'effici',\n",
       " 'gear',\n",
       " 'sodium',\n",
       " '19',\n",
       " 'variabl',\n",
       " 'take',\n",
       " 'mold',\n",
       " 'matrix',\n",
       " 'laser',\n",
       " 'queri',\n",
       " '220',\n",
       " 'track',\n",
       " 'pin',\n",
       " 'inject',\n",
       " 'gene',\n",
       " 'δ',\n",
       " 'subsequ',\n",
       " 'board',\n",
       " 'wash',\n",
       " '36',\n",
       " 'exist',\n",
       " 'action',\n",
       " 'sever',\n",
       " 'updat',\n",
       " '52',\n",
       " 'pad',\n",
       " 'convent',\n",
       " 'bu',\n",
       " 'secondari',\n",
       " 'hand',\n",
       " 'adhes',\n",
       " 'regard',\n",
       " 'approxim',\n",
       " 'ident',\n",
       " 'employ',\n",
       " 'around',\n",
       " 'error',\n",
       " 'salt',\n",
       " 'administ',\n",
       " 'common',\n",
       " 'variat',\n",
       " 'silicon',\n",
       " 'potenti',\n",
       " 'rf',\n",
       " 'audio',\n",
       " 'wheel',\n",
       " 'polypeptid',\n",
       " 'longitudin',\n",
       " 'regist',\n",
       " '103',\n",
       " 'predict',\n",
       " 'ch',\n",
       " 'vari',\n",
       " 'entiti',\n",
       " 'acquir',\n",
       " 'exchang',\n",
       " 'row',\n",
       " 'assign',\n",
       " 'detector',\n",
       " 'varieti',\n",
       " 'pump',\n",
       " 'packag',\n",
       " 'handl',\n",
       " 'orient',\n",
       " 'neg',\n",
       " 'serv',\n",
       " 'deposit',\n",
       " 'although',\n",
       " 'discharg',\n",
       " 'chip',\n",
       " 'dielectr',\n",
       " '122',\n",
       " 'stir',\n",
       " '204',\n",
       " 'suffici',\n",
       " 'flexibl',\n",
       " 'caviti',\n",
       " 'stop',\n",
       " 'diseas',\n",
       " 'run',\n",
       " 'natur',\n",
       " 'isol',\n",
       " 'hold',\n",
       " 'window',\n",
       " 'fabric',\n",
       " 'interv',\n",
       " 'feed',\n",
       " '35',\n",
       " 'convers',\n",
       " '310',\n",
       " 'densiti',\n",
       " 'insid',\n",
       " 'articl',\n",
       " 'identif',\n",
       " 'disk',\n",
       " 'q',\n",
       " 'onto',\n",
       " 'cabl',\n",
       " 'cycl',\n",
       " 'crystal',\n",
       " 'relationship',\n",
       " 'pipe',\n",
       " 'coil',\n",
       " '160',\n",
       " 'nozzl',\n",
       " 'tag',\n",
       " 'long',\n",
       " 'qualiti',\n",
       " 'methyl',\n",
       " 'wave',\n",
       " 'deliveri',\n",
       " 'furthermor',\n",
       " 'return',\n",
       " 'evalu',\n",
       " 'analyz',\n",
       " 'amplifi',\n",
       " 'coordin',\n",
       " 'still',\n",
       " 'hydrogen',\n",
       " 'appreci',\n",
       " 'compress',\n",
       " 'nmr',\n",
       " 'al',\n",
       " 'curv',\n",
       " 'blood',\n",
       " 'four',\n",
       " 'dose',\n",
       " 'complex',\n",
       " '206',\n",
       " 'entir',\n",
       " 'dna',\n",
       " '116',\n",
       " 'among',\n",
       " 'outsid',\n",
       " 'share',\n",
       " 'strength',\n",
       " 'wt',\n",
       " 'roller',\n",
       " 'blade',\n",
       " 'consid',\n",
       " 'room',\n",
       " 'entri',\n",
       " '302',\n",
       " 'pharmaceut',\n",
       " 'rule',\n",
       " '51',\n",
       " 'membran',\n",
       " 'radiat',\n",
       " 'cach',\n",
       " 'min',\n",
       " 'slot',\n",
       " 'formul',\n",
       " 'replac',\n",
       " 'horizont',\n",
       " 'page',\n",
       " 'next',\n",
       " 'mask',\n",
       " '111',\n",
       " 'ion',\n",
       " 'height',\n",
       " 'nucleotid',\n",
       " '212',\n",
       " 'establish',\n",
       " 'game',\n",
       " 'platform',\n",
       " 'spring',\n",
       " 'rear',\n",
       " 'catalyst',\n",
       " 'enter',\n",
       " 'custom',\n",
       " 'origin',\n",
       " 'bar',\n",
       " 'delay',\n",
       " '44',\n",
       " 'z',\n",
       " 'receptor',\n",
       " 'web',\n",
       " 'bear',\n",
       " 'trigger',\n",
       " 'rail',\n",
       " 'administr',\n",
       " 'facilit',\n",
       " 'equival',\n",
       " 'presenc',\n",
       " 'abl',\n",
       " 'expand',\n",
       " 'aqueou',\n",
       " 'automat',\n",
       " 'implant',\n",
       " 'wavelength',\n",
       " 'loop',\n",
       " 'free',\n",
       " 'divid',\n",
       " 'larger',\n",
       " 'introduc',\n",
       " 'led',\n",
       " 'fit',\n",
       " 'final',\n",
       " 'act',\n",
       " 'ink',\n",
       " 'offset',\n",
       " 'stabil',\n",
       " 'fill',\n",
       " 'across',\n",
       " 'ethyl',\n",
       " 'cpu',\n",
       " 'linear',\n",
       " 'authent',\n",
       " 'dynam',\n",
       " 'found',\n",
       " '38',\n",
       " 'context',\n",
       " 'build',\n",
       " 'accept',\n",
       " 'clock',\n",
       " 'branch',\n",
       " '33',\n",
       " 'opposit',\n",
       " 'sound',\n",
       " 'phenyl',\n",
       " 'groov',\n",
       " 'previous',\n",
       " 'score',\n",
       " 'ppm',\n",
       " 'label',\n",
       " 'thread',\n",
       " 'cross',\n",
       " '41',\n",
       " 'smaller',\n",
       " 'traffic',\n",
       " 'piec',\n",
       " 'inlet',\n",
       " 'dimens',\n",
       " 'optim',\n",
       " 'transmitt',\n",
       " '54',\n",
       " 'ph',\n",
       " 'visual',\n",
       " 'phone',\n",
       " 'rout',\n",
       " 'numer',\n",
       " 'intens',\n",
       " 'expos',\n",
       " 'involv',\n",
       " 'alloc',\n",
       " 'circuitri',\n",
       " 'apertur',\n",
       " 'wafer',\n",
       " 'ii',\n",
       " 'button',\n",
       " 'press',\n",
       " 'onc',\n",
       " 'repeat',\n",
       " 'necessari',\n",
       " 'interior',\n",
       " 'random',\n",
       " 'powder',\n",
       " 'advanc',\n",
       " '600',\n",
       " '45',\n",
       " 'bromid',\n",
       " 'capacitor',\n",
       " 'elong',\n",
       " '62',\n",
       " 'differenti',\n",
       " 'stack',\n",
       " 'symbol',\n",
       " 'peak',\n",
       " 'cost',\n",
       " 'recess',\n",
       " 'outlet',\n",
       " 'constant',\n",
       " '115',\n",
       " 'cancer',\n",
       " 'pivot',\n",
       " 'problem',\n",
       " '320',\n",
       " 'molecular',\n",
       " 'observ',\n",
       " 'write',\n",
       " 'travel',\n",
       " '410',\n",
       " '27',\n",
       " 'depth',\n",
       " 'tip',\n",
       " '201',\n",
       " 'sent',\n",
       " 'deliv',\n",
       " 'etch',\n",
       " 'bone',\n",
       " 'fuel',\n",
       " '230',\n",
       " 'reach',\n",
       " 'gap',\n",
       " 'dispers',\n",
       " 'schedul',\n",
       " 'partit',\n",
       " 'moreov',\n",
       " 'transit',\n",
       " 'public',\n",
       " 'consum',\n",
       " 'retain',\n",
       " 'peripher',\n",
       " 'minimum',\n",
       " '250',\n",
       " 'portabl',\n",
       " 'ground',\n",
       " 'reduct',\n",
       " '304']"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words = sorted(bags.items(), key= lambda x: x[1], reverse=True)[:1000]\n",
    "bag_of_words = [w[0] for w in bag_of_words]\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# build bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def label2int(lab):\n",
    "    return LABEL_MAPPING[lab]\n",
    "\n",
    "def vectorizer(input_text, bag_of_words):\n",
    "    \n",
    "    vec = []\n",
    "    for w in bag_of_words: \n",
    "        if w in word_tokenize(input_text): \n",
    "            vec.append('1') \n",
    "        else: \n",
    "            vec.append('0') \n",
    "    return vec\n",
    "            \n",
    "# def term_frequency():\n",
    "\n",
    "# def tf_idf():\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAPPING = {\n",
    "    'Patent Expired Due to NonPayment of Maintenance Fees Under 37 CFR 1': '1',\n",
    "    'Patented Case': '1',\n",
    "    'Final Rejection Mailed': '0',\n",
    "    'Expressly Abandoned  --  During Publication Process': '0',\n",
    "    'Expressly Abandoned  --  During Examination': '0',\n",
    "    'Abandonment for Failure to Correct Drawings': '0',\n",
    "    'Abandoned  --  Incomplete Application ': '0',\n",
    "    'Abandoned  --  Failure to Respond to an Office Action': '0',\n",
    "    'Abandoned  --  Failure to Pay Issue Fee': '0',\n",
    "    'Abandoned  --  After Examiner': '0'\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-254-2b0058ebd7f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mfull_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabstract\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdetailedDescription\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msummaryOfInvention\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mfeature_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_of_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mvalid_lab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel2int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-245-a1ea1ea8ca92>\u001b[0m in \u001b[0;36mvectorizer\u001b[0;34m(input_text, bag_of_words)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbag_of_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     return [\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     ]\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     return [\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     ]\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Handles parentheses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/re.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(match, template)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;31m# literal replacement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "features  = []\n",
    "for data in read_text('sample_from_full.json'):\n",
    "    #     print(data)    \n",
    "    label = get_label(data)\n",
    "    label_candi = LABEL_MAPPING.keys()\n",
    "    \n",
    "    if (len(label) == 1) and (label[0] in label_candi):\n",
    "\n",
    "        abstract = get_field_values(data, 'abstract')\n",
    "        detailedDescription = get_field_values(data, 'detailedDescription')\n",
    "        summaryOfInvention = get_field_values(data, 'summaryOfInvention')\n",
    "        full_text = abstract + ' ' + detailedDescription + ' ' + summaryOfInvention + '\\n'\n",
    "        \n",
    "        feature_vec = vectorizer(full_text, bag_of_words)\n",
    "        \n",
    "        valid_lab = label2int(label[0])\n",
    "        \n",
    "        \n",
    "        with open('vectorized.txt', 'a') as fh:\n",
    "            line =  ','.join(feature_vec) + ',' + valid_lab + '\\n'\n",
    "            fh.write(line)\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1     2     3     4     5     6     7     8     9     ...  991   \\\n",
       "0      1     0     1     0     0     0     1     1     1     1  ...     0   \n",
       "1      1     0     1     0     0     0     1     1     1     1  ...     0   \n",
       "2      1     0     1     0     0     0     0     1     1     1  ...     0   \n",
       "3      1     0     0     0     0     0     1     0     1     1  ...     0   \n",
       "4      1     0     1     0     0     0     1     1     1     1  ...     0   \n",
       "5      1     0     1     0     0     0     0     0     1     1  ...     0   \n",
       "6      1     0     1     0     0     0     0     1     1     1  ...     0   \n",
       "7      1     0     1     0     0     0     1     1     1     1  ...     0   \n",
       "8      1     0     1     0     0     0     1     1     1     1  ...     0   \n",
       "9      1     0     1     0     0     0     1     0     1     1  ...     0   \n",
       "10     1     0     1     0     0     0     1     1     1     1  ...     0   \n",
       "11     1     0     1     0     0     0     1     1     1     1  ...     0   \n",
       "\n",
       "    992   993   994   995   996   997   998   999   1000  \n",
       "0      0     0     0     0     0     0     0     0     1  \n",
       "1      1     0     0     0     0     0     0     0     1  \n",
       "2      0     0     1     0     0     0     0     0     1  \n",
       "3      0     0     0     0     0     0     0     0     1  \n",
       "4      0     0     0     0     0     0     0     0     0  \n",
       "5      0     0     0     0     0     0     0     0     0  \n",
       "6      0     0     0     0     0     0     0     0     1  \n",
       "7      0     0     1     1     0     0     0     1     1  \n",
       "8      0     0     0     0     0     0     0     1     0  \n",
       "9      1     0     0     0     0     0     0     0     0  \n",
       "10     0     0     0     0     0     0     0     0     1  \n",
       "11     0     0     0     0     0     1     0     0     1  \n",
       "\n",
       "[12 rows x 1001 columns]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('vectorized.txt', header  = None, sep = ',')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
